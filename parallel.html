<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Workshop on Savio, Parallel R, and Data Storage</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Workshop on Savio, Parallel R, and Data Storage</h1>

<h2>November 7, 2016</h2>

<p>Chris Paciorek, Department of Statistics and 
Berkeley Research Computing, UC Berkeley</p>

<h1>0) This workshop</h1>

<p>This tutorial covers parallelization in R, use of Savio (and Linux clusters in general), and campus resources for computing and data storage. </p>

<p>Savio is the campus Linux high-performance computing cluster, run by <a href="http://research-it.berkeley.edu/programs/berkeley-research-computing">Berkeley Research Computing</a>.</p>

<p>This tutorial assumes you have a working knowledge of R. </p>

<p>Materials for this tutorial, including the R markdown file and associated code files that were used to create this document are available on Github at <a href="https://github.com/berkeley-scf/ph295-parallel-2016">https://github.com/berkeley-scf/ph295-parallel-2016</a>.  You can download the files by doing a git clone from a terminal window on a UNIX-like machine, as follows:</p>

<pre><code class="r">git clone https://github.com/berkeley-scf/ph295-parallel-2016
</code></pre>

<p>The materials are also available as a <a href="https://github.com/berkeley-scf/ph295-parallel-2016/archive/master.zip">zip file</a>.</p>

<p>To create this HTML document, simply compile the corresponding R Markdown file in R as follows.</p>

<pre><code class="r">Rscript -e &quot;library(knitr); knit2html(&#39;parallel.Rmd&#39;)&quot;
</code></pre>

<p>or use the <em>Makefile</em> that is part of the repository.</p>

<p>This material by Christopher Paciorek is licensed under a Creative Commons Attribution 3.0 Unported License.</p>

<h1>1) Learning resources and links</h1>

<p>This workshop is based in part on already-prepared BRC and SCF (Statistical Computing Facility) material and other documentation that you can look at for more details:</p>

<ul>
<li><a href="http://research-it.berkeley.edu/services/high-performance-computing">Instructions for using the Savio campus Linux cluster</a></li>
<li><a href="https://github.com/ucberkeley/savio-training-intro-2016">Tutorial on basic usage of Savio</a>, in particular the <a href="https://rawgit.com/ucberkeley/savio-training-intro-2016/master/intro.html">HTML overview</a></li>
<li><a href="https://github.com/ucberkeley/savio-training-parallel-2016">Tutorial on parallelization on Savio</a>, in particular the <a href="https://rawgit.com/ucberkeley/savio-training-parallel-2016/master/parallel.html">HTML overview</a></li>
<li><a href="https://github.com/berkeley-scf/tutorial-parallel-basics">Tutorial on shared memory parallel processing</a>, in particular the <a href="https://rawgit.com/berkeley-scf/tutorial-parallel-basics/master/parallel-basics.html">HTML overview</a></li>
<li><a href="https://github.com/berkeley-scf/tutorial-parallel-distributed">Tutorial on distributed memory parallel processing</a>, in particular the <a href="https://rawgit.com/berkeley-scf/tutorial-parallel-distributed/master/parallel-dist.html">HTML overview</a></li>
</ul>

<h1>2) Computational and big data storage resources</h1>

<h2>2a) Computing</h2>

<ul>
<li><a href="http://research-it.berkeley.edu/services/high-performance-computing">Savio campus cluster</a>

<ul>
<li>~6600 nodes across ~330 nodes, 64 GB RAM on most nodes but up to 512 GB RAM on high-memory nodes</li>
<li>access through faculty computing allowance or departmental/faculty condos (biostat faculty have 8 nodes in a condo)</li>
<li>Python, R, MATLAB, Spark</li>
</ul></li>
<li><a href="https://www.xsede.org">NSF XSEDE network</a> of supercomputers

<ul>
<li>Bridges supercomputer for big data computing, including Spark</li>
<li>many other clusters/supercomputers</li>
</ul></li>
<li>Biostatistics 

<ul>
<li>Linux cluster: 8 nodes x 24 cores/node; 64 Gb RAM per node; SGE queueing</li>
</ul></li>
</ul>

<p><strong>Big picture: if you don&#39;t have enough computing resources, don&#39;t give up and work on a smaller problem, talk to us at <a href="mailto:brc@berkeley.edu">brc@berkeley.edu</a>.</strong></p>

<h1>2) Computational and big data storage resources</h1>

<h2>2b) Data storage</h2>

<ul>
<li>Unlimited storage in scratch on Savio (old files purged eventually)</li>
<li>Savio &ldquo;condo&rdquo; storage: $7000 for 25 TB for 5 years</li>
<li>Unlimited storage on Box through Berkeley (15 GB file limit)</li>
<li>Unlimited storage on Google Drive (bDrive) through Berkeley (5 TB file limit)</li>
</ul>

<p>See this <a href="http://research-it.berkeley.edu/services/high-performance-computing/transferring-data">Savio link for details on automating transfer into and out of Box and Drive</a>. </p>

<h1>3) Parallel processing overview</h1>

<h2>3a) Parallel processing terminology</h2>

<ul>
<li><em>cores</em>: We&#39;ll use this term to mean the different processing
units available on a single node.</li>
<li><em>nodes</em>: We&#39;ll use this term to mean the different computers,
each with their own distinct memory, that make up a cluster or supercomputer.</li>
<li><em>processes</em> or <em>SLURM tasks</em>: computational instances executing on a machine; multiple
processes may be executing at once. Ideally we have no more processes than cores on
a node.</li>
<li><em>threads</em>: multiple paths of execution within a single process;
the OS sees the threads as a single process, but one can think of
them as &#39;lightweight&#39; processes. Ideally when considering the processes
and their threads, we would have no more processes and threads combined
than cores on a node.

<ul>
<li><em>computational tasks</em>: We&#39;ll use this to mean the independent computational units that make up the job you submit</li>
<li>each <em>process</em> or <em>SLURM task</em> might carry out one computational task or might be assigned multiple tasks sequentially or as a group.</li>
</ul></li>
</ul>

<h1>3) Parallel processing overview</h1>

<h2>3b) Parallel processing approaches</h2>

<p>The easiest situation is when your code is <em>embarrassingly parallel</em>,
which means that the different tasks can be done independently and
the results collected. When the tasks need to interact, things get
much harder. Much of the material here is focused on embarrassingly
parallel computation.</p>

<p>In R we can:</p>

<ul>
<li>work in shared memory using multiple cores on a single machine (<em>foreach</em> with the <em>doParallel</em> backend, <em>mclapply</em>, <em>parLapply</em>, etc.)

<ul>
<li>work in shared memory using multiple cores via threaded linear algebra (with a threaded BLAS such as <em>openBLAS</em> or <em>MKL</em> linked to R)</li>
<li>work in distributed memory across multiple nodes on multiple machines (<em>foreach</em> with <em>doMPI</em> or <em>doSNOW</em>, <em>parLapply</em> with <em>snow</em>, <em>pbdR</em>)</li>
</ul></li>
</ul>

<h1>4) Savio / Linux cluster basics</h1>

<h2>4a) Overview</h2>

<p>The basic steps for using Savio (or similar resources) are:</p>

<ul>
<li><a href="https://docs.google.com/a/berkeley.edu/forms/d/e/1FAIpQLSemSeqZFI5jCvu4KxdQ_7MpQmZaAy57JNjOoGrSay6XbIU9qg/viewform">getting an account</a>

<ul>
<li>through your affiliation with the biostatistics big data training grant</li>
<li>through your advisor&#39;s faculty computing allowance</li>
<li>through a condo purchased by your advisor/group/department</li>
</ul></li>
<li><a href="http://research-it.berkeley.edu/services/high-performance-computing/logging-savio">logging on with a one-time password via the Pledge software</a>. Note that the password procedure will be changing shortly.</li>
<li><a href="http://research-it.berkeley.edu/services/high-performance-computing/transferring-data">transferring data to Savio</a></li>
<li><a href="http://research-it.berkeley.edu/services/high-performance-computing/accessing-and-installing-software">loading software modules or installing any software you need</a></li>
<li><a href="http://research-it.berkeley.edu/services/high-performance-computing/running-your-jobs">submitting your job(s)</a></li>
</ul>

<h1>4) Savio / Linux cluster basics</h1>

<h2>4b) Logging on to Savio</h2>

<p>To login, you need to have software on your own machine that gives you access to a UNIX terminal (command-line) session. These come built-in with Mac (see <code>Applications -&gt; Utilities -&gt; Terminal</code>). For Windows, some options include <a href="http://www.chiark.greenend.org.uk/%7Esgtatham/putty/download.html">PuTTY</a>.</p>

<p>You also need to set up your smartphone or tablet with <em>Google Authenticator</em> to generate one-time passwords for you.</p>

<p>Here are instructions for <a href="http://research-it.berkeley.edu/services/high-performance-computing/logging-savio">doing this setup, and for logging in</a>.</p>

<p>Then to login:</p>

<pre><code>ssh SAVIO_USERNAME@hpc.brc.berkeley.edu
</code></pre>

<p>Then enter XXXXXYYYYYY where XXXXXX is your PIN and YYYYYY is the one-time password. YYYYYY will be shown when you open your <em>Google authenticator</em> app on your phone/tablet.</p>

<p>One can then navigate around and get information using standard UNIX commands such as <code>ls</code>, <code>cd</code>, <code>du</code>, <code>df</code>, etc.</p>

<p>If you want to be able to open programs with graphical user interfaces:</p>

<pre><code>ssh -Y SAVIO_USERNAME@hpc.brc.berkeley.edu
</code></pre>

<p>To display the graphical windows on your local machine, you&#39;ll need X server software on your own machine to manage the graphical windows. For Windows, your options include <em>eXceed</em> or <em>Xming</em> and for Mac, there is <em>XQuartz</em>.</p>

<p>To log on to other Linux clusters/machines, often all you need to do is the <code>ssh</code> step above, with a simple fixed password.</p>

<h1>4) Savio / Linux cluster basics</h1>

<h2>4c) Data transfer using the command line</h2>

<p>We can use the <em>scp</em> and <em>sftp</em> protocols to transfer files.</p>

<p>You need to use the Savio data transfer node, <code>dtn.brc.berkeley.edu</code>.</p>

<h3>Linux/Mac:</h3>

<pre><code># to Savio, while on your local machine
scp bayArea.csv paciorek@dtn.brc.berkeley.edu:~/.
scp bayArea.csv paciorek@dtn.brc.berkeley.edu:~/data/newName.csv
scp bayArea.csv paciorek@dtn.brc.berkeley.edu:/global/scratch/paciorek/.

# from Savio, while on your local machine
scp paciorek@dtn.brc.berkeley.edu:~/data/newName.csv ~/Desktop/.
</code></pre>

<p>If you can ssh to your local machine or want to transfer files to other systems on to which you can ssh, you can use syntax like this, while logged onto Savio:</p>

<pre><code>ssh dtn
scp ~/file.csv OTHER_USERNAME@other.domain.edu:~/data/.
</code></pre>

<h3>Windows and GUI-based programs</h3>

<p>One program you can use with Windows is <em>WinSCP</em>, and a multi-platform program for doing transfers via SFTP is <em>FileZilla</em>. After logging in, you&#39;ll see windows for the Savio filesystem and your local filesystem on your machine. You can drag files back and forth.</p>

<p>You can package multiple files (including directory structure) together using tar:</p>

<pre><code>tar -cvzf files.tgz dir_to_zip file_to_zip file2_to_zip
# to untar later:
tar -xvzf files.tgz
</code></pre>

<h1>4) Savio / Linux cluster basics</h1>

<h2>4d) Accessing software</h2>

<p>A lot of software is available on Savio but needs to be loaded from the relevant software module before you can use it.</p>

<pre><code>module list  # what&#39;s loaded?
module avail  # what&#39;s available
</code></pre>

<p>One thing that tricks people is that the modules are arranged in a hierarchical (nested) fashion, so you only see some of the modules as being available <em>after</em> you load the parent module. Here&#39;s how we see the R packages that are available.</p>

<pre><code>which R
R

module avail
module load R
which R
module avail
module load Rmpi ggplot2
R
# library(ggplot2)
</code></pre>

<h2>Sidenote: installing h2o and other packages</h2>

<pre><code>module load java r ggplot2
module load Rmpi # for doMPI
R
</code></pre>

<pre><code class="r"># needed for h2o:
install.packages(c(&#39;RCurl&#39;, &#39;jsonlite&#39;, &#39;statmod&#39;))
# install h2o itself
install.packages(&quot;h2o&quot;, type=&quot;source&quot;, 
  repos=(c(&quot;http://h2o-release.s3.amazonaws.com/h2o/rel-turing/9/R&quot;)))
library(h2o)
localH2O = h2o.init(nthreads=-1)
demo(h2o.kmeans)

# for demo below, also want:
install.packages(c(&#39;caret&#39;, &#39;mlbench&#39;, &#39;cvAUC&#39;, &#39;devtools&#39;))
withr::with_libpaths(new = Sys.getenv(&quot;R_LIBS_USER&quot;), 
 devtools::install_github(&quot;h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package&quot;))

install.packages(c(&#39;SuperLearner&#39;))

install.packages(c(&#39;doMPI&#39;))

# where are packages installed?
.libPaths()
</code></pre>

<pre><code>ls /global/home/users/${USER}/R/x86_64-pc-linux-gnu-library/3.2
</code></pre>

<h1>4) Savio / Linux cluster basics</h1>

<h2>4e) Submitting jobs</h2>

<p>All computations are done by submitting jobs to the scheduling software that manages jobs on the cluster, called SLURM.</p>

<p>When submitting a job, the main things you need to indicate are the project account you are using (in some cases you might have access to multiple accounts such as a faculty computing allowance (FCA) and a condo) and the partition.</p>

<p>You can see what accounts you have access to and which partitions within those accounts as follows:</p>

<pre><code>sacctmgr -p show associations user=${USER}
</code></pre>

<p>Let&#39;s see how to submit a simple job. If your job will only use the resources on a single node, you can do the following. </p>

<p>Here&#39;s an example job script <em>job-example.sh</em> that I&#39;ll run. To run your own job, you&#39;ll need to modify the <em>&ndash;account</em> value and possibly the <em>&ndash;partition</em> value.</p>

<pre><code>#!/bin/bash
# Job name:
#SBATCH --job-name=test
#
# Account:
#SBATCH --account=co_stat
#
# Partition:
#SBATCH --partition=savio2
#
# Wall clock limit (30 seconds here):
#SBATCH --time=00:00:30
#
## Command(s) to run:
module load java r ggplot2
R CMD BATCH --no-save test_h2o.R test_h2o.Rout
</code></pre>

<p>Now let&#39;s submit and monitor the job:</p>

<pre><code>sbatch job-example.sh

squeue -u paciorek

squeue -j JOB_ID

wwall -j JOB_ID

# to see everything that is running or queued
squeue
</code></pre>

<p>To see error and output messages, look in any output files that your code creates and in <em>slurm-JOB_ID.out</em>.</p>

<p>Note that except for the <em>savio2_htc</em>  and <em>savio2_gpu</em> partitions, all jobs are given exclusive access to the entire node or nodes assigned to the job (and your account is charged for all of the cores on the node(s)). </p>

<h1>4) Savio / Linux cluster basics</h1>

<h2>4f)  Interactive jobs</h2>

<p>You can also do work interactively.</p>

<p>For this, you may want to have used the -Y flag to ssh if you are running software with a GUI such as MATLAB or wanting to be able to display plots in R or similar software. </p>

<pre><code># ssh -Y SAVIO_USERNAME@hpc.brc.berkeley.edu
srun -A co_stat -p savio2  --nodes=1 --time=00:10:00 --pty bash
# now execute on the compute node:
module load java r
R
</code></pre>

<pre><code class="r">library(h2o)
localH2O = h2o.init(nthreads=-1)
demo(h2o.kmeans)
</code></pre>

<h1>5) Basic parallel R on one machine/node</h1>

<p>One key thing to note here is that for parallelizing independent iterations/replications of a computation (as in Sections 5 and 6 here) there are a zillion ways to do this in R, with a variety of functions you can use (foreach, parLapply, mclapply) and a variety of parallel functionality behind the scenes (MPI, SNOW, sockets, pbdR). They&#39;ll all likely have similar computation time, so whatever you can make work is likely to be fine. </p>

<p>Here&#39;s how <em>CV.SuperLearner</em> parallelizes over folds:</p>

<pre><code>    if (parallel == &quot;seq&quot;) {
       cvList &lt;- lapply(folds, FUN = .crossValFun, Y = Y, dataX = X, 
            family = family, SL.library = SL.library, method = method, 
            id = id, obsWeights = obsWeights, verbose = verbose, 
            control = control, cvControl = cvControl, saveAll = saveAll)
    }
    else if (parallel == &quot;multicore&quot;) {
        .SL.require(&quot;parallel&quot;)
        cvList &lt;- parallel::mclapply(folds, FUN = .crossValFun, 
            Y = Y, dataX = X, family = family, SL.library = SL.library, 
            method = method, id = id, obsWeights = obsWeights, 
            verbose = verbose, control = control, cvControl = cvControl, 
            saveAll = saveAll, mc.set.seed = FALSE)
    }
    else if (inherits(parallel, &quot;cluster&quot;)) {
        cvList &lt;- parallel::parLapply(parallel, x = folds, fun = .crossValFun, 
            Y = Y, dataX = X, family = family, SL.library = SL.library, 
            method = method, id = id, obsWeights = obsWeights, 
            verbose = verbose, control = control, cvControl = cvControl, 
            saveAll = saveAll)
    }
</code></pre>

<h1>5a) Basic example we&#39;ll use</h1>

<p>In the examples below, I&#39;ll illustrate use of various parallel functionality in R using simple leave-one-out cross-validation for a Random Forest. </p>

<p>Here&#39;s the basic code we&#39;ll use:</p>

<pre><code class="r">looFit &lt;- function(i, Y, X, family = gaussian()) {
    out &lt;- SL.randomForest(Y[-i], X[-i, ], X[i, ], family)
    return(out$pred)
}

set.seed(23432)
## training set
n &lt;- 500
p &lt;- 50
X &lt;- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) &lt;- paste(&quot;X&quot;, 1:p, sep=&quot;&quot;)
X &lt;- data.frame(X)
Y &lt;- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)
</code></pre>

<h1>5) Basic parallel R on one machine/node</h1>

<h2>5b) job submission</h2>

<p>Here&#39;s an example job submission script, <em>job-onenode.sh</em>.</p>

<pre><code>#!/bin/bash
# Job name:
#SBATCH --job-name=test
#
# Account:
#SBATCH --account=co_stat
#
# Partition:
#SBATCH --partition=savio2
#
# Number of nodes:
#SBATCH --nodes=1
#
# Wall clock limit (30 seconds here):
#SBATCH --time=00:00:30
#
## Command(s) to run:
module load r
R CMD BATCH --no-save mclapply.R mclapply.Rout
</code></pre>

<p>Now simply invoke:</p>

<pre><code>sbatch job-onenode.sh
</code></pre>

<h1>5) Basic parallel R on one machine/node</h1>

<h2>5c) Using  SLURM environment variables to control your job</h2>

<p>When you write your code, you may need to specify information in your code about the number of cores to use. SLURM will provide a variety of variables that you can use in your code so that it adapts to the resources you have requested rather than being hard-coded. </p>

<p>Here are some of the variables that may be useful: SLURM_NTASKS, SLURM_CPUS_PER_TASK, SLURM_CPUS_ON_NODE, SLURM_NODELIST, SLURM_NNODES.</p>

<p>Here&#39;s how you can access those variables in your R code:</p>

<pre><code>as.numeric(Sys.getenv(&#39;SLURM_NTASKS&#39;))  # invoke this in R
</code></pre>

<p>To use multiple cores on a node (and thereby fully utilize the node that will be exclusively assigned to your job), be careful if you only specify <em>&ndash;nodes</em>, as the environment variables will only indicate one task per node.</p>

<h1>5) Basic parallel R on one machine/node</h1>

<h2>5d) mclapply</h2>

<p>Here&#39;s how we can apply LOO for this simulated dataset seen above.</p>

<pre><code>library(parallel)
cores &lt;- as.numeric(Sys.getenv(&#39;SLURM_CPUS_ON_NODE&#39;))
preds &lt;- mclapply(seq_len(n), looFit, Y = Y, X = X, mc.cores = cores)
</code></pre>

<p>Note that <em>mclapply</em> <em>forks</em> the master process (makes copies of the process), so all loaded packages and objects will be automatically present in the worker processes.</p>

<h1>5) Basic parallel R on one machine/node</h1>

<h2>5e) parLapply</h2>

<pre><code>library(parallel)
cores &lt;- as.numeric(Sys.getenv(&#39;SLURM_CPUS_ON_NODE&#39;))
cl &lt;- makeCluster(cores) 
preds &lt;- parLapply(cl, seq_len(n), looFit, Y, X, TRUE)
</code></pre>

<p><em>makeCluster</em> starts up new processes so one needs to reload the needed packages on each worker (the TRUE gets passed to as the <em>loadLib</em> argument to <em>looFit()</em>. Also if one used a variable from the global environment of the master session, you would need to use <em>clusterExport</em> to make it available. For example if <em>X</em> and <em>Y</em> were used in <em>loo()</em> without being arguments, you would have needed:</p>

<pre><code>clusterExport(cl, c(&#39;X&#39;, &#39;Y&#39;))
</code></pre>

<h1>5) Basic parallel R on one machine/node</h1>

<h2>5f) foreach</h2>

<pre><code>library(doParallel)

cores &lt;- as.numeric(Sys.getenv(&#39;SLURM_CPUS_ON_NODE&#39;))
registerDoParallel(cores)
preds &lt;- foreach(i = seq_len(n)) %dopar% { 
      looFit(i, Y, X) 
}
</code></pre>

<h1>6) Basic parallel R on multiple nodes</h1>

<h2>6a) Job submission</h2>

<p>If you are submitting a job that uses multiple nodes, you&#39;ll need to carefully specify the resources you need. The key flags for use in your job script are:</p>

<ul>
<li><code>--nodes</code> (or <code>-N</code>): indicates the number of nodes to use</li>
<li><code>--ntasks-per-node</code>: indicates the number of SLURM tasks (i.e., processes) one wants to run on each node</li>
<li><code>--cpus-per-task</code> (or <code>-c</code>): indicates the number of cpus to be used for each task</li>
</ul>

<p>In addition, in some cases it can make sense to use the <code>--ntasks</code> (or <code>-n</code>) option to indicate the total number of tasks and let the scheduler determine how many nodes and tasks per node are needed. In general <code>--cpus-per-task</code> will be 1 except when running threaded code.  </p>

<p>Here&#39;s an example job script (<em>job-multinode.sh</em>) for a job that uses MPI for parallelizing over multiple nodes:</p>

<pre><code>#!/bin/bash
# Job name:
#SBATCH --job-name=test
#
# Account:
#SBATCH --account=co_stat
#
# Partition:
#SBATCH --partition=savio2
#
# Number of MPI tasks needed for use case (example):
#SBATCH --ntasks=48
#
# Processors per task:
#SBATCH --cpus-per-task=1
#
# Wall clock limit (15 minutes here):
#SBATCH --time=00:15:00
#
## Command(s) to run (example):
module load gcc openmpi r 
module load Rmpi
mpirun R CMD BATCH --no-save foreach-multinode-doMPI.R \
   foreach-multinode-doMPI.Rout
</code></pre>

<h1>6) Basic parallel R on multiple nodes</h1>

<h2>6b) foreach with doMPI</h2>

<pre><code>source(&#39;loo.R&#39;)
library(doMPI)

cl = startMPIcluster()  # default starts one fewer slave than available cores
registerDoMPI(cl)

preds &lt;- foreach(i = seq_len(n), .packages = &#39;SuperLearner&#39;) %dopar% { 
      looFit(i, Y, X) 
}

closeCluster(cl)
mpi.quit()
</code></pre>

<p>Note that in some cases in which you are not using MPI through SLURM, you may need to indicate the number of MPI processes MPI should start and on what machines they should run. See <a href="https://github.com/berkeley-scf/tutorial-parallel-distributed">my tutorial on distributed memory parallel processing</a>.</p>

<h1>6) Basic parallel R on multiple nodes</h1>

<h2>6c) foreach and parLapply with SNOW clusters</h2>

<p>One nice thing about SNOW is that it can avoid having to use MPI, which can be a hassle to setup and maintain (though SNOW can also make use of MPI). So in this case, don&#39;t start R via <em>mpirun</em> in your job submission script.</p>

<p>To use SNOW in this fashion, we&#39;ll need to tell SNOW the machines on which to run the workers and how many workers to run on each machine. Here&#39;s an example where I know the machines I want to use in advance.</p>

<p>(NOTE to self: demonstrate on arwen not smeagol)</p>

<pre><code>source(&quot;loo.R&quot;)
library(snow)
machines = c(rep(&quot;beren.berkeley.edu&quot;, 1),
    rep(&quot;gandalf.berkeley.edu&quot;, 1),
    rep(&quot;arwen.berkeley.edu&quot;, 2))

cl = makeCluster(machines, type = &quot;SOCK&quot;)
</code></pre>

<p>Now I can either use that cluster with <em>foreach</em> or with <em>parLapply</em>.</p>

<pre><code>library(doSNOW)
registerDoSNOW(cl)

preds &lt;- foreach(i = seq_len(n), .packages = &#39;SuperLearner&#39;) %dopar% { 
      looFit(i, Y, X) 
}

stopCluster(cl)
</code></pre>

<pre><code>library(parallel)
preds &lt;- parLapply(cl, seq_len(n), looFit, Y, X, TRUE)

stopCluster(cl)
</code></pre>

<h1>7) Parallelization strategies</h1>

<h2>7a) Basic principles</h2>

<p>The following are some basic principles/suggestions for how to parallelize
your computation.</p>

<h1>Should I use one machine/node or many machines/nodes?</h1>

<ul>
<li>If you can do your computation on the cores of a single node using
shared memory, that will be faster than using the same number of cores
(or even somewhat more cores) across multiple nodes. 

<ul>
<li>Similarly, jobs with a lot of data/high memory requirements that one might think of
as requiring Spark or Hadoop may in some cases be much faster if you can find
a single machine with a lot of memory.</li>
</ul></li>
<li>That said, if you would run out of memory on a single node, then you&#39;ll
need to use distributed memory.</li>
</ul>

<h1>What level or dimension should I parallelize over?</h1>

<ul>
<li>If you have nested loops, you generally only want to parallelize at
one level of the code. That said, there may be cases in which it is
helpful to do both. Keep in mind whether your linear algebra is being
threaded. Often you will want to parallelize over a loop and not use
threaded linear algebra.</li>
<li>Often it makes sense to parallelize the outer loop when you have nested
loops.</li>
<li>You generally want to parallelize in such a way that your code is
load-balanced and does not involve too much communication. </li>
</ul>

<h1>How do I balance communication overhead with keeping my cores busy?</h1>

<ul>
<li>If you have very few tasks, particularly if the tasks take different
amounts of time, often some of the processors will be idle and your code
poorly load-balanced.</li>
<li>If you have very many tasks and each one takes little time, the communication
overhead of starting and stopping the tasks will reduce efficiency.</li>
</ul>

<h1>Should multiple tasks be pre-assigned to a process (i.e., a worker) (sometimes called <em>prescheduling</em>) or should tasks be assigned dynamically as previous tasks finish?</h1>

<ul>
<li>Basically if you have many tasks that each take similar time, you
want to preschedule the tasks to reduce communication. If you have few tasks
or tasks with highly variable completion times, you don&#39;t want to
preschedule, to improve load-balancing.</li>
<li>For R in particular, some of R&#39;s parallel functions allow you to say whether the 
tasks should be prescheduled. E.g., <code>library(parallel); help(mclapply)</code> gives some information.</li>
</ul>

<h1>7) Parallelization strategies</h1>

<h2>7b) Strategy discussion for targeted learning</h2>

<p>Parallelization could be over:</p>

<ul>
<li>cross-validation folds</li>
<li>algorithms</li>
<li>within an algorithm (linear algebra or other threading, e.g., for internal cross-validation)</li>
</ul>

<p>Let&#39;s discuss how we might split up our computations based on how many computational tasks there are for each dimension of parallelization, how long tasks take, and how varied the computational time is.</p>

<h1>8) Getting help</h1>

<ul>
<li>For technical issues and questions about using Savio: 

<ul>
<li><a href="mailto:brc-hpc-help@berkeley.edu">brc-hpc-help@berkeley.edu</a></li>
</ul></li>
<li>For questions about computing resources in general, including cloud computing: 

<ul>
<li><a href="mailto:brc@berkeley.edu">brc@berkeley.edu</a></li>
</ul></li>
<li>For questions about data management (including HIPAA-protected data): 

<ul>
<li><a href="mailto:researchdata@berkeley.edu">researchdata@berkeley.edu</a></li>
</ul></li>
</ul>

</body>

</html>
